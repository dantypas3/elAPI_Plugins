"""
This module defines: class:`BaseImporter`, an abstract base class for
importing data into ElabFTW endpoints.  It provides shared helpers for
normalizing identifiers, resolving category columns, extracting titles,
parsing tag lists, and updating ``metadata.extra_fields`` on existing
resources or experiments.

Concrete importers should subclass: class:`BaseImporter` and implement
the :attr:`df`, :attr:`cols_lower` and :attr:`endpoint` properties.
"""

from __future__ import annotations

import json
import logging
import math
import mimetypes
import os
from abc import ABC, abstractmethod
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Union

import pandas as pd
from elapi.api import FixedEndpoint
from responses import Response

from src.utils.common import canonicalize, ensure_series
from src.utils.paths import RES_IMPORTER_CONFIG

try:
  with open(RES_IMPORTER_CONFIG, "r", encoding="utf-8") as config_file:
    CONFIG = json.load(config_file)
except FileNotFoundError:
  raise FileNotFoundError(
    f"Config file not found. Tried: {RES_IMPORTER_CONFIG}. "
    f"Set RES_IMPORTER_CONFIG to override, or ensure config/res_importer_config.json exists at repo root.")
except json.JSONDecodeError as e:
  raise ValueError(f"Error decoding JSON from {RES_IMPORTER_CONFIG}: {e}")

logger = logging.getLogger(__name__)


class BaseImporter(ABC):
  """Abstract base class for resource and experiment importers.

  Subclasses must expose three properties:
  This base class implements generic helpers such as normalising ids,
  extracting category ids, titles, and tags, and updating extra fields in
  metadata.  Concrete importers can build upon these to provide more
  specialised behaviour.
  """

  # ------------------- Canonicalization & column helpers -------------------

  def _canonicalize_column_indexes(self, columns: pd.Index) -> Dict[str, str]:

    canon_column_map: Dict[str, str] = {}

    for original_name in columns:
      if not isinstance(original_name, str):
        continue

      canon_lower_key = canonicalize(original_name.lower())

      if canon_lower_key in canon_column_map and canon_lower_key != original_name:
        logger.debug("Canonicalized lowercase column collision: %r vs %r for key %r",
                     canon_column_map[canon_lower_key], original_name, canon_lower_key)
      else:
        canon_column_map.setdefault(canon_lower_key, original_name)

    return canon_column_map

  def _find_path_col(self) -> Optional[str]:
    """Find a column name that matches any known file path aliases."""
    return next(
      (col for col in self.cols_canon.values()
       if col in CONFIG["path_col"]),
      None
    )

  def _normalize_date(self, row: pd.Series) -> Optional[str]:

    if not ensure_series(row):
      return None

    date_col = self.cols_canon.get("date")

    if date_col is None or date_col not in row.index:
      return None

    date_val = row[date_col]

    if pd.isna(date_val):
      return None

    date_str = str(date_val).strip()
    if not date_str:
      return None

    for pattern in CONFIG["date_patterns"]:
      try:
        dt = datetime.strptime(date_str, pattern)
        return dt.strftime("%Y-%m-%d")
      except ValueError:
        continue

    logger.warning("Unrecognized date format: %r", date_str)
    return None

    # --- Required API ---

  @property
  @abstractmethod
  def basic_df(self) -> pd.DataFrame:
    """Return the DataFrame backing the importer."""
    raise NotImplementedError

  @property
  @abstractmethod
  def cols_canon(self) -> Dict[str, str]:
    """Return a mapping from lower‑case canonicalized column names to the original names."""
    raise NotImplementedError

  @property
  @abstractmethod
  def endpoint(self) -> FixedEndpoint:
    """Return the endpoint used to interact with ElabFTW."""
    raise NotImplementedError

  @property
  def files_base_dir(self) -> Optional[Path]:
    """Optional base directory for resolving relative file paths."""
    return None

  @property
  def df(self) -> pd.DataFrame:
    """Alias for the backing DataFrame used by some importers."""
    return self.basic_df

  def _find_col_like(self, name: str) -> Optional[str]:
    """Find a column whose canonical form matches or contains ``name``."""
    target = canonicalize(name)
    if target in self.cols_canon:
      return self.cols_canon[target]
    for canon_col, original in self.cols_canon.items():
      if target in canon_col or canon_col in target:
        return original
    return None

    # ------------------- files helpers -------------------

  def _iter_files_in_dir(self, folder: Union[str, Path], recursive: bool = True) -> List[Path]:

    path = Path(str(folder)).expanduser()

    if not path.exists() or not path.is_dir():
      logger.warning(
        "Files folder does not exist or is not a directory: %s", path)
      return []

    if recursive:
      files = [f for f in path.rglob("*") if f.is_file()]
    else:
      files = [f for f in path.iterdir() if f.is_file()]

    if not files:
      logger.warning("No files found in folder: %s", path)

    return files

  def _resolve_folder(self, raw_value: Union[str, Path]) -> Optional[Path]:
    """
    Convert a CSV cell into a Path if it looks like a valid file/folder path.
    Returns None if the value is empty, invalid, or clearly not a path.
    """
    if raw_value is None or (
      isinstance(raw_value, float) and pd.isna(raw_value)):
      return None

    # Convert to string, normalize whitespace and non-breaking spaces
    path_str = str(raw_value).replace("\u00a0", " ").strip()
    if not path_str:
      return None

    looks_like_path = (any(
      ch in path_str for ch in (os.sep, "/", "\\")) or os.path.isabs(
      path_str) or "." in os.path.basename(
      path_str) or self.files_base_dir is not None)

    if not looks_like_path:
      logger.info(
        "Skipping files upload: value does not look like a path: %r",
        path_str)
      return None

    resolved_path = Path(path_str).expanduser()

    if not resolved_path.is_absolute() and self.files_base_dir:
      resolved_path = (self.files_base_dir / resolved_path).resolve()

    return resolved_path

  def _attach_files(
    self, resource_id: Union[int, str],
    folder: Union[str, Path], recursive: bool = True,
    chunk_size: int = 10
  ) -> None:

    res_id = str(resource_id)
    if not res_id.isdigit():
      raise ValueError(
        f"Invalid resource ID for upload: {resource_id!r}")

    logger.info("Uploading files for resource %s from %s", res_id, folder)
    files = self._iter_files_in_dir(folder, recursive=recursive)
    if not files:
      logger.warning("No files to upload from: %s", folder)
      return

    def _mime_or_default(p: Path) -> str:
      return mimetypes.guess_type(p.name)[
        0] or "application/octet-stream"

    def _send_batch(batch: List[Path]) -> bool:
      payload = []
      handles = []
      try:
        for fp in batch:
          fh = fp.open("rb")
          handles.append(fh)
          payload.append(
            ("files[]", (fp.name, fh, _mime_or_default(fp))))
        try:
          resp = self.endpoint.post(endpoint_id=res_id,
                                    sub_endpoint_name="uploads",
                                    files=payload)
          resp.raise_for_status()
          logger.debug("Uploaded batch of %d files for resource %s", len(batch), res_id)
          return True
        except Exception as exc:
          logger.info("Batched upload (%d files) failed: %s",
                      len(batch), exc)
          return False
      finally:
        for h in handles:
          try:
            h.close()
          except Exception:
            pass

    if chunk_size and chunk_size > 1:
      i = 0
      all_batches_ok = True
      while i < len(files):
        batch = files[i: i + chunk_size]
        ok = _send_batch(batch)
        if not ok:
          all_batches_ok = False
          break
        i += chunk_size
      if all_batches_ok:
        logger.info("Uploaded %d files in %d batch(es).", len(files),
                    (len(files) + chunk_size - 1) // chunk_size)
        return

    errors: List[str] = []
    for fp in files:
      try:
        logger.debug("Uploading file %s to resource %s", fp, res_id)
        with fp.open("rb") as fh:
          resp = self.endpoint.post(endpoint_id=res_id,
                                    sub_endpoint_name="uploads",
                                    files=[("files[]", (fp.name, fh,
                                                        _mime_or_default(
                                                          fp)))])
        try:
          resp.raise_for_status()
          continue
        except Exception:
          with fp.open("rb") as fh2:
            resp2 = self.endpoint.post(endpoint_id=res_id,
                                       sub_endpoint_name="uploads",
                                       files={
                                         "file": (fp.name, fh2,
                                                  _mime_or_default(
                                                    fp))
                                       })
          resp2.raise_for_status()
      except Exception as exc:
        logger.error("Failed to upload file %s to resource %s: %s", fp,
                     res_id, exc)
        errors.append(f"{fp}: {exc}")

    if errors:
      raise RuntimeError(
        "One or more uploads failed:\n- " + "\n- ".join(errors))

  # --- Category handling ---
  def validate_category_id(self, cid: str) -> None:
    """Validate that a category ID is numeric."""

    if not str(cid).isdigit():
      raise ValueError("Category ID must be numeric.")

  def resolve_category_col(self) -> Optional[str]:
    """Try to identify the column storing category ids from the DataFrame."""

    for c in self.basic_df.columns:
      if canonicalize(c).startswith(
        "categoryid") or "categoryid" in canonicalize(c):
        return c
    return None

  def normalize_id(self, value: Any) -> Optional[str]:
    """Return a normalised identifier or ``None`` if the value is empty."""
    if value is None:
      return None
    if isinstance(value, float) and math.isnan(value):
      return None
    if isinstance(value, float) and value.is_integer():
      value = int(value)
    s = str(value).strip()
    if s.lower() in {"", "nan", "none", "null"}:
      return None
    return s

  def get_category_id(self, row: pd.Series) -> Optional[str]:
    """Extract and validate the category id from a row.

    If the category column is missing or the value is empty, ``None`` is
    returned.  Otherwise the id is normalised and validated as numeric.

    :param row: A row from the CSV as a Series.
    :returns: The category id or ``None`` if unavailable.
    :raises ValueError: If the id is non‑numeric.
    """
    if row is None or not isinstance(row, pd.Series):
      return None
    col = self.resolve_category_col()
    if col is None or col not in row:
      return None
    cid = self.normalize_id(row[col])
    if cid is None:
      return None
    self.validate_category_id(cid)
    return cid

  # --- HTTP helpers ---
  def get_elab_id(self, response: Response) -> str:
    """Extract the numeric identifier from a ``Location`` header.

    :param response: The HTTP response from a POST request.
    :returns: A numeric identifier as a string.
    :raises RuntimeError: If the id cannot be parsed.
    """
    location = str(response.headers.get("Location"))
    exp_id = location.rstrip("/").split("/")[-1]
    if not exp_id.isdigit():
      raise RuntimeError(f"Could not parse experiment ID: {exp_id!r}")
    return exp_id

  # --- Column extraction ---
  def _get_title(self, row: pd.Series) -> Optional[str]:
    """Return the title value from a row."""

    if row is None:
      return None
    if not isinstance(row, pd.Series):
      try:
        row = pd.Series(row._asdict())
      except Exception:
        return None

    title_col = self.cols_canon.get("title")

    if not title_col or title_col not in row:
      return None

    title_val = row[title_col]

    if title_val is None or str(title_val).strip() == "":
      return None

    return str(title_val).strip()

  def get_tags(self, row: pd.Series) -> List[str]:
    """Parse the tags column from a row into a list.

    Tags may be stored as a list/tuple/set, a pipe/semicolon/comma separated
    string, or any scalar value.  Empty or missing tags return an empty
    list.

    :param row: A row from the CSV.
    :returns: A list of tags.
    """
    if row is None:
      return []
    tags_col = self.cols_canon.get("tags")
    if not tags_col or tags_col not in row:
      return []
    val = row[tags_col]
    if pd.isna(val):
      return []
    if isinstance(val, (list, tuple, set)):
      return [str(x).strip() for x in val if str(x).strip()]
    if isinstance(val, str):

      for delim in [";", ",", "|"]:
        if delim in val:
          parts = val.split(delim)
          break
      else:
        parts = [val]
      return [p.strip() for p in parts if p.strip()]
    s = str(val).strip()
    if not s or s.lower() in {"nan", "none", "null"}:
      return []
    return [s]

  # --- Existing record helpers ---
  def get_existing_json(self, elab_id: str) -> Dict[str, Any]:
    """Retrieve the existing JSON representation for an id.

    :param elab_id: The identifier of the resource or experiment.
    :returns: A dictionary representing the JSON record; empty if an
      error occurs or no JSON is returned.
    """
    try:
      logger.debug("Fetching existing JSON for id %s", elab_id)
      response = self.endpoint.get(endpoint_id=elab_id)
      response_json = response.json()
      if isinstance(response_json, dict):
        return response_json
    except Exception as exc:
      logger.warning("Failed to fetch existing JSON for id %s: %s",
                     elab_id, exc)
    return {}

  def fetch_extra_fields_mapping(self, elab_json: Dict[str, Any]) -> Dict[
    str, Dict[str, Any]]:
    """Return a mapping from canonicalised extra field titles to their definitions.

    :param elab_json: The existing JSON record containing ``metadata_decoded``.
    :returns: A mapping keyed by canonical titles.
    """
    metadata_decoded = elab_json.get("metadata_decoded", {})
    extra_fields = metadata_decoded.get("extra_fields", [])
    mapping: Dict[str, Dict[str, Any]] = {}
    if isinstance(extra_fields, list):
      for field in extra_fields:
        if not isinstance(field, dict):
          continue
        title = field.get("title") or field.get("slug") or field.get(
          "name")
        if title:
          mapping[canonicalize(title)] = field
    return mapping

  def update_extra_fields_from_row(
    self, elab_id: str, row: pd.Series,
    known_columns: Iterable[str]
  ) -> None:
    """Patch extra fields on the existing record using values from the CSV row.

    Any CSV column whose canonicalised name matches an existing extra field
    title (and is not listed in ``known_columns``) will be used to update
    that field's ``value``.  Missing values are converted to the empty
    string.

    :param elab_id: The identifier of the resource/experiment to update.
    :param row: A CSV row as a Series.
    :param known_columns: A collection of canonicalised names to skip (e.g.
      ``"title"``, ``"tags"``).
    """
    existing = self.get_existing_json(elab_id)
    if not existing:
      return
    extra_map = self.fetch_extra_fields_mapping(existing)
    if not extra_map:
      return
    updated_fields: List[str] = []
    for column in row.index:
      canon_col = canonicalize(column)
      if canon_col in extra_map and canon_col not in known_columns:
        value = row[column]
        if pd.isna(value):
          val_str = ""
        else:
          if hasattr(value, "item") and not isinstance(value, str):
            try:
              value = value.item()
            except Exception:
              pass
          val_str = str(value)
        extra_map[canon_col]["value"] = val_str
        updated_fields.append(canon_col)
    patch_data = {
      "metadata": {
        "extra_fields": list(extra_map.values())
      }
    }
    try:
      logger.debug("Patching extra fields for id %s: %s", elab_id, updated_fields)
      response = self.endpoint.patch(endpoint_id=elab_id,
                                     data=patch_data)
      response.raise_for_status()
    except Exception as exc:
      logger.error("Failed to patch extra fields for id %s: %s", elab_id,
                   exc)
